{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "project_root = Path().absolute().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "import os\n",
        "os.chdir(project_root)\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "News enabled: True\n",
            "Tickers: ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META']\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from src.data.pipeline_with_news import get_datasets_with_news\n",
        "from src.models.transformer_model_with_news import StockTransformerWithNews\n",
        "from src.training.trainer_with_news import TrainerWithNews\n",
        "from src.utils.config import load_config\n",
        "\n",
        "config = load_config()\n",
        "\n",
        "# Enable news in config\n",
        "config.data.use_news = True\n",
        "\n",
        "print(f\"News enabled: {config.data.use_news}\")\n",
        "print(f\"Tickers: {config.data.tickers}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading datasets with news embeddings...\n",
            "–ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –ª–æ–∫–∞–ª–µ–Ω dataset –æ—Ç: data\\raw\\sp500_stocks_data.parquet\n",
            "–ó–∞—Ä–µ–¥–µ–Ω–æ! –†–∞–∑–º–µ—Ä: (1048575, 23)\n",
            "–§–∏–ª—Ç—Ä–∏—Ä–∞–Ω–µ –Ω–∞ –¥–∞–Ω–Ω–∏... –ü—ä—Ä–≤–æ–Ω–∞—á–∞–ª–µ–Ω —Ä–∞–∑–º–µ—Ä: (1048575, 25)\n",
            "–§–∏–ª—Ç—Ä–∏—Ä–∞–Ω–æ! –§–∏–Ω–∞–ª–µ–Ω —Ä–∞–∑–º–µ—Ä: (10068, 25)\n",
            "\n",
            "üì∞ Extracting news embeddings...\n",
            "Loading FinBERT encoder...\n",
            "Loading FinBERT model: ProsusAI/finbert\n",
            "Device: cpu\n",
            "‚úÖ FinBERT loaded successfully\n",
            "üì¶ Loading cached news embeddings for AAPL from data\\processed\\news_cache\\AAPL_20100105_20200930.pkl\n",
            "üì¶ Loading cached news embeddings for AMZN from data\\processed\\news_cache\\AMZN_20100105_20200930.pkl\n",
            "üì¶ Loading cached news embeddings for GOOGL from data\\processed\\news_cache\\GOOGL_20100105_20200930.pkl\n",
            "üì¶ Loading cached news embeddings for MSFT from data\\processed\\news_cache\\MSFT_20100105_20200930.pkl\n",
            "‚úÖ News embeddings extracted: (4, 770)\n",
            "‚úÖ Datasets loaded!\n",
            "  Train: 6876 samples\n",
            "  Val: 1426 samples\n",
            "  Test: 1427 samples\n",
            "  Features: 34\n"
          ]
        }
      ],
      "source": [
        "# Load datasets with news\n",
        "print(\"Loading datasets with news embeddings...\")\n",
        "train_dataset, val_dataset, test_dataset, feature_columns = get_datasets_with_news(\n",
        "    config=config,\n",
        "    use_news_cache=True,\n",
        "    force_refresh_news=False  # Set to True to refresh news cache\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Datasets loaded!\")\n",
        "print(f\"  Train: {len(train_dataset)} samples\")\n",
        "print(f\"  Val: {len(val_dataset)} samples\")\n",
        "print(f\"  Test: {len(test_dataset)} samples\")\n",
        "print(f\"  Features: {len(feature_columns)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch format: 3 items\n",
            "  x shape: torch.Size([64, 60, 34])\n",
            "  news_emb shape: torch.Size([64, 768])\n",
            "  y shape: torch.Size([64, 1])\n"
          ]
        }
      ],
      "source": [
        "# Create data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config.training.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    pin_memory=False,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config.training.batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=False,\n",
        ")\n",
        "\n",
        "# Test batch to check news embeddings\n",
        "sample_batch = next(iter(train_loader))\n",
        "print(f\"Batch format: {len(sample_batch)} items\")\n",
        "if len(sample_batch) == 3:\n",
        "    x, news_emb, y = sample_batch\n",
        "    print(f\"  x shape: {x.shape}\")\n",
        "    print(f\"  news_emb shape: {news_emb.shape if news_emb is not None else None}\")\n",
        "    print(f\"  y shape: {y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Model created!\n",
            "  Input dim: 34\n",
            "  News embedding dim: 768\n",
            "  Fusion method: concat\n"
          ]
        }
      ],
      "source": [
        "# Create enhanced model with news\n",
        "model = StockTransformerWithNews(\n",
        "    input_dim=len(feature_columns),\n",
        "    news_embedding_dim=768,  # FinBERT embedding dimension\n",
        "    d_model=config.model.d_model,\n",
        "    n_heads=config.model.n_heads,\n",
        "    n_layers=config.model.n_layers,\n",
        "    d_ff=config.model.d_ff,\n",
        "    dropout=config.model.dropout,\n",
        "    activation=config.model.activation,\n",
        "    prediction_horizon=config.data.prediction_horizon,\n",
        "    news_fusion_method=\"concat\",  # or \"add\"\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Model created!\")\n",
        "print(f\"  Input dim: {len(feature_columns)}\")\n",
        "print(f\"  News embedding dim: 768\")\n",
        "print(f\"  Fusion method: concat\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Trainer created!\n"
          ]
        }
      ],
      "source": [
        "# Create trainer\n",
        "trainer = TrainerWithNews(\n",
        "    model=model,\n",
        "    config=config,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Trainer created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "–ó–∞–ø–æ—á–≤–∞–Ω–µ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∑–∞ 50 epochs...\n",
            "\n",
            "Epoch 1/50... Train loss: 0.075509 Val loss: 0.068093 (18.8s)\n",
            "\n",
            "Epoch 2/50... Train loss: 0.019546 Val loss: 0.046746 (18.4s)\n",
            "\n",
            "Epoch 3/50... Train loss: 0.016253 Val loss: 0.053059 (18.3s)\n",
            "\n",
            "Epoch 4/50... Train loss: 0.014535 Val loss: 0.045858 (17.9s)\n",
            "\n",
            "Epoch 5/50... Train loss: 0.013542 Val loss: 0.026436 (18.4s)\n",
            "\n",
            "Epoch 6/50... Train loss: 0.013780 Val loss: 0.027150 (18.4s)\n",
            "\n",
            "Epoch 7/50... Train loss: 0.013078 Val loss: 0.030975 (18.2s)\n",
            "\n",
            "Epoch 8/50... Train loss: 0.012565 Val loss: 0.022738 (18.1s)\n",
            "\n",
            "Epoch 9/50... Train loss: 0.012717 Val loss: 0.025734 (18.1s)\n",
            "\n",
            "Epoch 10/50... Train loss: 0.012480 Val loss: 0.021866 (17.8s)\n",
            "\n",
            "Epoch 11/50... Train loss: 0.014767 Val loss: 0.023496 (18.0s)\n",
            "\n",
            "Epoch 12/50... Train loss: 0.012088 Val loss: 0.019637 (18.0s)\n",
            "\n",
            "Epoch 13/50... Train loss: 0.011921 Val loss: 0.025595 (19.2s)\n",
            "\n",
            "Epoch 14/50... Train loss: 0.011808 Val loss: 0.017069 (18.4s)\n",
            "\n",
            "Epoch 15/50... Train loss: 0.011347 Val loss: 0.019506 (17.9s)\n",
            "\n",
            "Epoch 16/50... Train loss: 0.011089 Val loss: 0.017083 (18.5s)\n",
            "\n",
            "Epoch 17/50... Train loss: 0.010842 Val loss: 0.018128 (18.0s)\n",
            "\n",
            "Epoch 18/50... Train loss: 0.010875 Val loss: 0.017353 (18.3s)\n",
            "\n",
            "Epoch 19/50... Train loss: 0.010839 Val loss: 0.024646 (18.0s)\n",
            "\n",
            "Early stopping triggered after 19 epochs (patience: 5)\n",
            "\n",
            "–û–±—É—á–µ–Ω–∏–µ—Ç–æ –∑–∞–≤—ä—Ä—à–∏ —É—Å–ø–µ—à–Ω–æ!\n",
            "Best validation loss: 0.017069\n",
            "\n",
            "============================================================\n",
            "–û–±—É—á–µ–Ω–∏–µ—Ç–æ –∑–∞–≤—ä—Ä—à–∏!\n",
            "Best validation loss: 0.017069\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "history = trainer.train()\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"–û–±—É—á–µ–Ω–∏–µ—Ç–æ –∑–∞–≤—ä—Ä—à–∏!\")\n",
        "print(f\"Best validation loss: {history['best_val_loss']:.6f}\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "–ó–∞–ø–∞–∑–≤–∞–Ω–µ –Ω–∞ –º–æ–¥–µ–ª–∞ –≤: C:\\Users\\vyoto\\OneDrive\\Desktop\\CODE STUFF\\Stock price prediction\\models\\checkpoints\\best_model_with_news.pt\n",
            "‚úì –§–∞–π–ª—ä—Ç –µ –∑–∞–ø–∞–∑–µ–Ω —É—Å–ø–µ—à–Ω–æ!\n",
            "  –†–∞–∑–º–µ—Ä: 5.89 MB\n",
            "  –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä–∞–Ω: Fri Feb 13 20:30:30 2026\n"
          ]
        }
      ],
      "source": [
        "# Save enhanced model\n",
        "from src.utils import config as _cfg\n",
        "\n",
        "checkpoint_name = \"best_model_with_news.pt\"\n",
        "checkpoint_path = _cfg.PROJECT_ROOT / config.paths.models_dir / checkpoint_name\n",
        "checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"\\n–ó–∞–ø–∞–∑–≤–∞–Ω–µ –Ω–∞ –º–æ–¥–µ–ª–∞ –≤: {checkpoint_path}\")\n",
        "torch.save({\n",
        "    'epoch': len(history['train_losses']) - 1,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'score': history['best_val_loss'],\n",
        "    'model_type': 'StockTransformerWithNews',\n",
        "    'config': config,\n",
        "}, checkpoint_path)\n",
        "\n",
        "if checkpoint_path.exists():\n",
        "    import time\n",
        "    file_size = checkpoint_path.stat().st_size / (1024 * 1024)  # MB\n",
        "    mtime = time.ctime(checkpoint_path.stat().st_mtime)\n",
        "    print(f\"‚úì –§–∞–π–ª—ä—Ç –µ –∑–∞–ø–∞–∑–µ–Ω —É—Å–ø–µ—à–Ω–æ!\")\n",
        "    print(f\"  –†–∞–∑–º–µ—Ä: {file_size:.2f} MB\")\n",
        "    print(f\"  –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä–∞–Ω: {mtime}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
