{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Ticker Backtest\n",
    "\n",
    "Tests the trading strategy on multiple unseen tickers and shows all results in one visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import os\n",
    "os.chdir(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.data.loader import load_and_filter_dataset\n",
    "from src.data.preprocessor import preprocess_data\n",
    "from src.data.feature_engineering import create_all_features\n",
    "from src.data.dataset import StockDataset, create_sequences, time_series_split\n",
    "from src.models.transformer_model import StockTransformer\n",
    "from src.simulation.engine import BacktestEngine\n",
    "from src.simulation.metrics import compute_metrics\n",
    "from src.utils.config import load_config\n",
    "from src.utils import config as _cfg\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = load_config()\n",
    "\n",
    "# List of tickers to test (NOT in training set)\n",
    "TRAINING_TICKERS = set(config.data.tickers)  # ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META']\n",
    "\n",
    "# Select 10 diverse tickers from different sectors\n",
    "TEST_TICKERS = [\n",
    "    \"DIS\",   # Entertainment\n",
    "    \"AMD\",   # Tech (semiconductors)\n",
    "    \"JPM\",   # Financials\n",
    "    \"JNJ\",   # Healthcare\n",
    "    \"WMT\",   # Retail\n",
    "    \"V\",     # Financials (payments)\n",
    "    \"INTC\",  # Tech (semiconductors)\n",
    "    \"CRM\",   # Tech (software)\n",
    "    \"NFLX\",  # Entertainment (streaming)\n",
    "    \"BAC\",   # Financials (banking)\n",
    "]\n",
    "\n",
    "print(f\"Training tickers: {sorted(TRAINING_TICKERS)}\")\n",
    "print(f\"\\nTesting on {len(TEST_TICKERS)} tickers:\")\n",
    "for i, ticker in enumerate(TEST_TICKERS, 1):\n",
    "    print(f\"  {i}. {ticker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load simulation config\n",
    "import yaml\n",
    "config_path = _cfg.PROJECT_ROOT / \"configs\" / \"default_config.yaml\"\n",
    "with config_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    raw_cfg = yaml.safe_load(f)\n",
    "sim = raw_cfg.get(\"simulation\", {})\n",
    "\n",
    "initial_capital = float(sim.get(\"initial_capital\", 100_000))\n",
    "position_size_pct = float(sim.get(\"position_size_pct\", 0.3))\n",
    "entry_threshold_pct = float(sim.get(\"entry_threshold_pct\", 0.5))\n",
    "exit_threshold_pct = float(sim.get(\"exit_threshold_pct\", -5.0))\n",
    "commission_pct = float(sim.get(\"commission_pct\", 0.1))\n",
    "risk_free_rate_annual = float(sim.get(\"risk_free_rate_annual\", 0.03))\n",
    "\n",
    "print(f\"Simulation config:\")\n",
    "print(f\"  Initial capital: ${initial_capital:,.0f}\")\n",
    "print(f\"  Position size: {position_size_pct*100:.0f}%\")\n",
    "print(f\"  Entry threshold: {entry_threshold_pct}%\")\n",
    "print(f\"  Exit threshold: {exit_threshold_pct}%\")\n",
    "print(f\"  Commission: {commission_pct}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "checkpoint_name = config.paths.checkpoint_file\n",
    "checkpoint_path = _cfg.PROJECT_ROOT / config.paths.models_dir / checkpoint_name\n",
    "\n",
    "print(f\"Loading model from: {checkpoint_path}\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "state_dict = checkpoint[\"model_state_dict\"]\n",
    "\n",
    "print(f\"Checkpoint: epoch {checkpoint.get('epoch', '?')}, val loss {checkpoint.get('score', '?'):.6f}\")\n",
    "\n",
    "# We'll determine input_dim from first ticker's data\n",
    "d_model = state_dict[\"input_projection.weight\"].shape[0]\n",
    "n_layers = len([k for k in state_dict if \"encoder.layers\" in k and \"self_attention.w_q.weight\" in k])\n",
    "n_heads = config.model.n_heads\n",
    "d_ff = state_dict[\"encoder.layers.0.feed_forward.linear1.weight\"].shape[0]\n",
    "\n",
    "print(f\"Model architecture: d_model={d_model}, n_layers={n_layers}, n_heads={n_heads}, d_ff={d_ff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original training data to fit scaler (same as model training)\n",
    "print(\"Loading original training data for scaler...\")\n",
    "from src.data.pipeline import extract_dataset\n",
    "df_train_original, feature_columns = extract_dataset(config=config)\n",
    "train_df_original, _, _ = time_series_split(\n",
    "    df_train_original,\n",
    "    train_split=config.data.train_split,\n",
    "    val_split=config.data.val_split,\n",
    "    test_split=config.data.test_split,\n",
    ")\n",
    "\n",
    "# Fit scaler on original training data\n",
    "all_numeric_cols = feature_columns + [\"close\"]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_df_original[all_numeric_cols])\n",
    "print(f\"Scaler fitted on {len(train_df_original)} training samples\")\n",
    "print(f\"Feature columns: {len(feature_columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model (input_dim will be set from feature_columns)\n",
    "input_dim = len(feature_columns)\n",
    "input_dim_checkpoint = state_dict[\"input_projection.weight\"].shape[1]\n",
    "\n",
    "if input_dim_checkpoint != input_dim:\n",
    "    print(f\"Input dim mismatch: checkpoint={input_dim_checkpoint}, current={input_dim}\")\n",
    "    print(\"Using compatible parameters only...\")\n",
    "    model = StockTransformer(\n",
    "        input_dim=input_dim,\n",
    "        d_model=d_model,\n",
    "        n_heads=n_heads,\n",
    "        n_layers=n_layers,\n",
    "        d_ff=d_ff,\n",
    "        dropout=config.model.dropout,\n",
    "        activation=config.model.activation,\n",
    "        prediction_horizon=config.data.prediction_horizon,\n",
    "    )\n",
    "    model_dict = model.state_dict()\n",
    "    compatible_dict = {k: v for k, v in state_dict.items()\n",
    "                      if k in model_dict and model_dict[k].shape == v.shape}\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict, strict=False)\n",
    "    print(f\"Loaded {len(compatible_dict)}/{len(state_dict)} params\")\n",
    "else:\n",
    "    model = StockTransformer(\n",
    "        input_dim=input_dim_checkpoint,\n",
    "        d_model=d_model,\n",
    "        n_heads=n_heads,\n",
    "        n_layers=n_layers,\n",
    "        d_ff=d_ff,\n",
    "        dropout=config.model.dropout,\n",
    "        activation=config.model.activation,\n",
    "        prediction_horizon=config.data.prediction_horizon,\n",
    "    )\n",
    "    model.load_state_dict(state_dict)\n",
    "    print(\"Model loaded successfully.\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test strategy on each ticker\n",
    "results = []\n",
    "\n",
    "for ticker in tqdm(TEST_TICKERS, desc=\"Testing tickers\"):\n",
    "    try:\n",
    "        # Load data for this ticker\n",
    "        test_config = deepcopy(config)\n",
    "        test_config.data.tickers = [ticker]\n",
    "        \n",
    "        df_raw = load_and_filter_dataset(\n",
    "            config=test_config,\n",
    "            tickers=[ticker],\n",
    "            start_date=test_config.data.start_date,\n",
    "            end_date=test_config.data.end_date,\n",
    "        )\n",
    "        \n",
    "        if len(df_raw) == 0:\n",
    "            print(f\"  No data for {ticker}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Preprocess\n",
    "        df_processed, _ = preprocess_data(\n",
    "            df_raw,\n",
    "            handle_missing=True,\n",
    "            missing_method=\"forward_fill\",\n",
    "            handle_outliers_flag=True,\n",
    "            outliers_method=\"clip\",\n",
    "            normalize=False,\n",
    "            date_column=\"date\",\n",
    "            symbol_column=\"symbol\",\n",
    "        )\n",
    "        \n",
    "        # Create features\n",
    "        df_features = create_all_features(\n",
    "            df_processed,\n",
    "            price_column=\"close\",\n",
    "            high_column=\"high\",\n",
    "            low_column=\"low\",\n",
    "            volume_column=\"volume\",\n",
    "            date_column=\"date\",\n",
    "            symbol_column=\"symbol\",\n",
    "            windows=test_config.data.features.windows,\n",
    "            lags=[1, 2, 3, 5, 10] if test_config.data.features.lag_features else [],\n",
    "            add_technical=test_config.data.features.technical_indicators,\n",
    "            add_lags=test_config.data.features.lag_features,\n",
    "            add_temporal=test_config.data.features.temporal_features,\n",
    "            add_volume=True,\n",
    "            simplified=test_config.data.features.simplified,\n",
    "        )\n",
    "        \n",
    "        df_features = df_features.dropna()\n",
    "        \n",
    "        if len(df_features) == 0:\n",
    "            print(f\"  No valid data after feature engineering for {ticker}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Apply scaler\n",
    "        df_features[all_numeric_cols] = scaler.transform(df_features[all_numeric_cols])\n",
    "        \n",
    "        # Split into train/val/test\n",
    "        train_df, val_df, test_df = time_series_split(\n",
    "            df_features,\n",
    "            train_split=test_config.data.train_split,\n",
    "            val_split=test_config.data.val_split,\n",
    "            test_split=test_config.data.test_split,\n",
    "        )\n",
    "        \n",
    "        if len(test_df) < config.data.context_length + 10:\n",
    "            print(f\"  Not enough test data for {ticker}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Create dataset\n",
    "        test_data = test_df[feature_columns].values\n",
    "        test_targets = test_df[\"close\"].values.reshape(-1, 1)\n",
    "        \n",
    "        test_X, test_y = create_sequences(\n",
    "            np.column_stack([test_data, test_targets]),\n",
    "            config.data.context_length,\n",
    "            config.data.prediction_horizon,\n",
    "        )\n",
    "        \n",
    "        test_X = test_X[:, :, :-1]\n",
    "        if config.data.prediction_horizon > 1:\n",
    "            test_y = test_y[:, :, -1]\n",
    "        else:\n",
    "            test_y = test_y[:, -1, -1]\n",
    "            if test_y.ndim == 0:\n",
    "                test_y = test_y.reshape(-1, 1)\n",
    "        \n",
    "        test_dataset = StockDataset(test_X, test_y, config.data.context_length, config.data.prediction_horizon)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "        \n",
    "        # Generate predictions\n",
    "        predictions = []\n",
    "        prices_list = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                features, targets = batch\n",
    "                preds = model(features)\n",
    "                predictions.append(preds.cpu().numpy())\n",
    "                prices_list.append(targets.cpu().numpy())\n",
    "        \n",
    "        pred_next = np.concatenate(predictions).ravel()\n",
    "        prices = np.concatenate(prices_list).ravel()\n",
    "        \n",
    "        # Align prices and predictions\n",
    "        test_prices = test_df[\"close\"].values\n",
    "        context_length = config.data.context_length\n",
    "        prices_for_backtest = test_prices[context_length:]\n",
    "        \n",
    "        min_len = min(len(pred_next), len(prices_for_backtest))\n",
    "        prices = prices_for_backtest[:min_len]\n",
    "        pred_next = pred_next[:min_len]\n",
    "        \n",
    "        # Run backtest\n",
    "        engine = BacktestEngine(\n",
    "            initial_capital=initial_capital,\n",
    "            position_size_pct=position_size_pct,\n",
    "            entry_threshold_pct=entry_threshold_pct,\n",
    "            exit_threshold_pct=exit_threshold_pct,\n",
    "            commission_pct=commission_pct,\n",
    "        )\n",
    "        \n",
    "        result = engine.run(prices=prices, predictions=pred_next)\n",
    "        \n",
    "        metrics = compute_metrics(\n",
    "            result,\n",
    "            initial_capital=initial_capital,\n",
    "            risk_free_rate_annual=risk_free_rate_annual,\n",
    "            prices=prices,\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            \"ticker\": ticker,\n",
    "            \"equity_curve\": result.equity_curve,\n",
    "            \"prices\": prices,\n",
    "            \"total_return_pct\": metrics.total_return_pct,\n",
    "            \"sharpe_ratio\": metrics.sharpe_ratio_annual,\n",
    "            \"max_drawdown_pct\": metrics.max_drawdown_pct,\n",
    "            \"num_trades\": metrics.num_trades,\n",
    "            \"buy_hold_return_pct\": metrics.buy_and_hold_return_pct,\n",
    "            \"excess_return_pct\": metrics.excess_return_vs_bh_pct,\n",
    "            \"final_equity\": result.equity_curve[-1],\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error processing {ticker}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n Successfully tested {len(results)} tickers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BACKTEST RESULTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTested {len(results)} tickers\")\n",
    "print(f\"\\n{'Ticker':<8} {'Return %':<12} {'B&H Return %':<15} {'Excess %':<12} {'Sharpe':<10} {'Max DD %':<12} {'Trades':<8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for _, row in df_results.iterrows():\n",
    "    print(f\"{row['ticker']:<8} {row['total_return_pct']:>10.2f}% {row['buy_hold_return_pct']:>13.2f}% {row['excess_return_pct']:>10.2f}% {row['sharpe_ratio']:>8.3f} {row['max_drawdown_pct']:>10.2f}% {row['num_trades']:>6}\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'AVERAGE':<8} {df_results['total_return_pct'].mean():>10.2f}% {df_results['buy_hold_return_pct'].mean():>13.2f}% {df_results['excess_return_pct'].mean():>10.2f}% {df_results['sharpe_ratio'].mean():>8.3f} {df_results['max_drawdown_pct'].mean():>10.2f}% {df_results['num_trades'].mean():>6.0f}\")\n",
    "\n",
    "# Win rate\n",
    "winning_trades = (df_results['total_return_pct'] > 0).sum()\n",
    "win_rate = winning_trades / len(df_results) * 100\n",
    "print(f\"\\nWin Rate: {winning_trades}/{len(results)} ({win_rate:.1f}%)\")\n",
    "\n",
    "# Beat buy & hold\n",
    "beat_bh = (df_results['excess_return_pct'] > 0).sum()\n",
    "beat_bh_rate = beat_bh / len(df_results) * 100\n",
    "print(f\"Beat Buy & Hold: {beat_bh}/{len(results)} ({beat_bh_rate:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all equity curves\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "ax1 = axes[0]\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(results)))\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    ticker = result['ticker']\n",
    "    equity = result['equity_curve']\n",
    "    return_pct = result['total_return_pct']\n",
    "    \n",
    "    # Normalize to start at 100k for comparison\n",
    "    equity_normalized = equity / equity[0] * initial_capital\n",
    "    \n",
    "    ax1.plot(equity_normalized, label=f\"{ticker} ({return_pct:+.1f}%)\", \n",
    "             color=colors[i], linewidth=2, alpha=0.8)\n",
    "\n",
    "ax1.axhline(y=initial_capital, color='black', linestyle='--', linewidth=1, alpha=0.5, label='Initial Capital')\n",
    "ax1.set_ylabel('Portfolio Value ($)', fontsize=12)\n",
    "ax1.set_title('Equity Curves - All Tickers', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='best', ncol=2, fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Second subplot: Returns comparison\n",
    "ax2 = axes[1]\n",
    "tickers = [r['ticker'] for r in results]\n",
    "strategy_returns = [r['total_return_pct'] for r in results]\n",
    "bh_returns = [r['buy_hold_return_pct'] if r['buy_hold_return_pct'] is not None else 0 for r in results]\n",
    "\n",
    "x = np.arange(len(tickers))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax2.bar(x - width/2, strategy_returns, width, label='Strategy', color='steelblue', alpha=0.8)\n",
    "bars2 = ax2.bar(x + width/2, bh_returns, width, label='Buy & Hold', color='orange', alpha=0.8)\n",
    "\n",
    "ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax2.set_xlabel('Ticker', fontsize=12)\n",
    "ax2.set_ylabel('Return (%)', fontsize=12)\n",
    "ax2.set_title('Returns Comparison: Strategy vs Buy & Hold', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(tickers)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show() \n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Average Strategy Return: {df_results['total_return_pct'].mean():.2f}%\")\n",
    "print(f\"Average Buy & Hold Return: {df_results['buy_hold_return_pct'].mean():.2f}%\")\n",
    "print(f\"Average Excess Return: {df_results['excess_return_pct'].mean():.2f}%\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}