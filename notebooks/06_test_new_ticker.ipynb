{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model on Unseen Ticker\n",
    "\n",
    "This notebook tests the trained model on a ticker that was NOT used during training.\n",
    "This tests the model's generalization ability.\n",
    "\n",
    "**Tickers used in training:** AAPL, MSFT, GOOGL, AMZN, META\n",
    "\n",
    "**New ticker to test:** NVDA (NVIDIA) - tech stock, similar sector but unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import os\n",
    "os.chdir(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.data.pipeline import get_datasets, extract_dataset\n",
    "from src.data.dataset import time_series_split\n",
    "from src.models.transformer_model import StockTransformer\n",
    "from src.simulation.engine import BacktestEngine\n",
    "from src.simulation.metrics import compute_metrics\n",
    "from src.utils.config import load_config, Config\n",
    "from src.utils import config as _cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config and modify for new ticker\n",
    "config = load_config()\n",
    "\n",
    "# ============================================================\n",
    "# CHANGE THIS TICKER TO TEST DIFFERENT STOCKS\n",
    "# ============================================================\n",
    "NEW_TICKER = \"CRM\"  # Disney - entertainment/media stock\n",
    "\n",
    "\n",
    "# Check what tickers are available (optional - just for info)\n",
    "from src.data.loader import load_raw_dataset\n",
    "print(\"Loading dataset to check available tickers...\")\n",
    "df_raw = load_raw_dataset(config=config)\n",
    "if \"symbol\" in df_raw.columns:\n",
    "    available_tickers = sorted(df_raw[\"symbol\"].unique())\n",
    "    print(f\"\\nAvailable tickers in dataset: {len(available_tickers)}\")\n",
    "    print(f\"First 30: {available_tickers[:30]}\")\n",
    "    \n",
    "    # Verify the ticker exists\n",
    "    if NEW_TICKER not in available_tickers:\n",
    "        print(f\"\\n  WARNING: {NEW_TICKER} not found in dataset!\")\n",
    "        print(f\"Available tech stocks: {[t for t in ['NVDA', 'TSLA', 'AMD', 'INTC', 'ORCL', 'CRM'] if t in available_tickers]}\")\n",
    "        raise ValueError(f\"Ticker {NEW_TICKER} not found. Choose from available tickers.\")\n",
    "    \n",
    "    # Check if ticker was used in training\n",
    "    training_tickers = set(config.data.tickers)\n",
    "    if NEW_TICKER in training_tickers:\n",
    "        print(f\"\\n  WARNING: {NEW_TICKER} was used in training!\")\n",
    "        print(f\"This won't test generalization. Choose a different ticker.\")\n",
    "else:\n",
    "    print(\"Warning: 'symbol' column not found.\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Testing on NEW ticker: {NEW_TICKER}\")\n",
    "print(f\"Tickers used in training: {config.data.tickers}\")\n",
    "print(f\"This ticker was NOT used during training - testing generalization!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for new ticker\n",
    "# IMPORTANT: We need to use the SAME scaler as training, not train a new one\n",
    "# So we'll load data WITHOUT normalization first, then apply the scaler from training\n",
    "\n",
    "from copy import deepcopy\n",
    "from src.data.loader import load_and_filter_dataset\n",
    "from src.data.preprocessor import preprocess_data\n",
    "from src.data.feature_engineering import create_all_features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"\\nLoading data for {NEW_TICKER}...\")\n",
    "\n",
    "# Step 1: Load raw data for new ticker\n",
    "test_config = deepcopy(config)\n",
    "test_config.data.tickers = [NEW_TICKER]\n",
    "\n",
    "df_raw = load_and_filter_dataset(\n",
    "    config=test_config,\n",
    "    tickers=[NEW_TICKER],\n",
    "    start_date=test_config.data.start_date,\n",
    "    end_date=test_config.data.end_date,\n",
    ")\n",
    "\n",
    "if len(df_raw) == 0:\n",
    "    raise ValueError(f\"No data found for {NEW_TICKER}. Available tickers: {available_tickers[:20]}\")\n",
    "\n",
    "print(f\"Raw data loaded: {len(df_raw)} rows\")\n",
    "\n",
    "# Step 2: Preprocess (handle missing, outliers) but DON'T normalize yet\n",
    "df_processed, _ = preprocess_data(\n",
    "    df_raw,\n",
    "    handle_missing=True,\n",
    "    missing_method=\"forward_fill\",\n",
    "    handle_outliers_flag=True,\n",
    "    outliers_method=\"clip\",\n",
    "    normalize=False,  # Don't normalize yet!\n",
    "    date_column=\"date\",\n",
    "    symbol_column=\"symbol\",\n",
    ")\n",
    "\n",
    "# Step 3: Create features\n",
    "df_features = create_all_features(\n",
    "    df_processed,\n",
    "    price_column=\"close\",\n",
    "    high_column=\"high\",\n",
    "    low_column=\"low\",\n",
    "    volume_column=\"volume\",\n",
    "    date_column=\"date\",\n",
    "    symbol_column=\"symbol\",\n",
    "    windows=test_config.data.features.windows,\n",
    "    lags=[1, 2, 3, 5, 10] if test_config.data.features.lag_features else [],\n",
    "    add_technical=test_config.data.features.technical_indicators,\n",
    "    add_lags=test_config.data.features.lag_features,\n",
    "    add_temporal=test_config.data.features.temporal_features,\n",
    "    add_volume=True,\n",
    "    simplified=test_config.data.features.simplified,\n",
    ")\n",
    "\n",
    "df_features = df_features.dropna()\n",
    "\n",
    "# Step 4: Get feature columns\n",
    "feature_columns = [\n",
    "    col for col in df_features.columns\n",
    "    if col not in [\"date\", \"symbol\", \"close\"]\n",
    "    and df_features[col].dtype in ['float64', 'int64', 'float32', 'int32']\n",
    "]\n",
    "\n",
    "print(f\"Features created: {len(feature_columns)}\")\n",
    "\n",
    "# Step 5: Load ORIGINAL training data to fit scaler (same as training)\n",
    "print(\"\\nLoading original training data to get scaler parameters...\")\n",
    "df_train_original, _ = extract_dataset(config=config)  # Uses original tickers\n",
    "train_df_original, _, _ = time_series_split(\n",
    "    df_train_original,\n",
    "    train_split=config.data.train_split,\n",
    "    val_split=config.data.val_split,\n",
    "    test_split=config.data.test_split,\n",
    ")\n",
    "\n",
    "# Step 6: Fit scaler on original training data (same as model training)\n",
    "all_numeric_cols = feature_columns + [\"close\"]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_df_original[all_numeric_cols])\n",
    "print(f\"Scaler fitted on {len(train_df_original)} training samples\")\n",
    "\n",
    "# Step 7: Apply scaler to new ticker data\n",
    "df_features[all_numeric_cols] = scaler.transform(df_features[all_numeric_cols])\n",
    "print(f\"Scaler applied to {NEW_TICKER} data\")\n",
    "\n",
    "# Step 8: Split into train/val/test (we'll use test split for backtest)\n",
    "train_df, val_df, test_df = time_series_split(\n",
    "    df_features,\n",
    "    train_split=test_config.data.train_split,\n",
    "    val_split=test_config.data.val_split,\n",
    "    test_split=test_config.data.test_split,\n",
    ")\n",
    "\n",
    "print(f\"\\nData loaded and normalized:\")\n",
    "print(f\"  Train: {len(train_df)} rows\")\n",
    "print(f\"  Val: {len(val_df)} rows\")\n",
    "print(f\"  Test: {len(test_df)} rows\")\n",
    "print(f\"  Features: {len(feature_columns)}\")\n",
    "\n",
    "if 'symbol' in test_df.columns:\n",
    "    print(f\"\\nTickers in test set: {sorted(test_df['symbol'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load simulation config\n",
    "config_path = _cfg.PROJECT_ROOT / \"configs\" / \"default_config.yaml\"\n",
    "with config_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    raw_cfg = yaml.safe_load(f)\n",
    "sim = raw_cfg.get(\"simulation\", {})\n",
    "\n",
    "initial_capital = float(sim.get(\"initial_capital\", 100_000))\n",
    "position_size_pct = float(sim.get(\"position_size_pct\", 0.3))\n",
    "entry_threshold_pct = float(sim.get(\"entry_threshold_pct\", 0.5))\n",
    "exit_threshold_pct = float(sim.get(\"exit_threshold_pct\", -5.0))\n",
    "commission_pct = float(sim.get(\"commission_pct\", 0.1))\n",
    "risk_free_rate_annual = float(sim.get(\"risk_free_rate_annual\", 0.03))\n",
    "\n",
    "print(f\"Simulation config: capital={initial_capital}, position_size={position_size_pct}, \")\n",
    "print(f\"entry_threshold={entry_threshold_pct}%, exit_threshold={exit_threshold_pct}%, commission={commission_pct}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model (trained on AAPL, MSFT, GOOGL, AMZN, META)\n",
    "checkpoint_name = config.paths.checkpoint_file\n",
    "checkpoint_path = _cfg.PROJECT_ROOT / config.paths.models_dir / checkpoint_name\n",
    "\n",
    "print(f\"Loading model from: {checkpoint_path}\")\n",
    "if checkpoint_path.exists():\n",
    "    import time\n",
    "    mod_time = time.ctime(checkpoint_path.stat().st_mtime)\n",
    "    print(f\"Loaded: {checkpoint_name} (modified {mod_time})\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "state_dict = checkpoint[\"model_state_dict\"]\n",
    "\n",
    "print(f\"Checkpoint: epoch {checkpoint.get('epoch', '?')}, val loss {checkpoint.get('score', '?'):.6f}\")\n",
    "\n",
    "# Handle input_dim mismatch (new ticker might have different feature count)\n",
    "input_dim_checkpoint = state_dict[\"input_projection.weight\"].shape[1]\n",
    "input_dim_current = len(feature_columns)\n",
    "d_model = state_dict[\"input_projection.weight\"].shape[0]\n",
    "n_layers = len([k for k in state_dict if \"encoder.layers\" in k and \"self_attention.w_q.weight\" in k])\n",
    "n_heads = config.model.n_heads\n",
    "d_ff = state_dict[\"encoder.layers.0.feed_forward.linear1.weight\"].shape[0]\n",
    "\n",
    "if input_dim_checkpoint != input_dim_current:\n",
    "    print(f\"\\nWARNING: Input dimension mismatch!\")\n",
    "    print(f\"  Checkpoint: {input_dim_checkpoint} features\")\n",
    "    print(f\"  Current data: {input_dim_current} features\")\n",
    "    print(f\"  This might happen if feature engineering differs.\")\n",
    "    print(f\"  Using compatible parameters only...\")\n",
    "    \n",
    "    model = StockTransformer(\n",
    "        input_dim=input_dim_current,\n",
    "        d_model=d_model,\n",
    "        n_heads=n_heads,\n",
    "        n_layers=n_layers,\n",
    "        d_ff=d_ff,\n",
    "        dropout=config.model.dropout,\n",
    "        activation=config.model.activation,\n",
    "        prediction_horizon=config.data.prediction_horizon,\n",
    "    )\n",
    "    model_dict = model.state_dict()\n",
    "    compatible_dict = {k: v for k, v in state_dict.items()\n",
    "                      if k in model_dict and model_dict[k].shape == v.shape}\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict, strict=False)\n",
    "    print(f\"Loaded {len(compatible_dict)}/{len(state_dict)} params (input_dim mismatch: using current features)\")\n",
    "else:\n",
    "    model = StockTransformer(\n",
    "        input_dim=input_dim_checkpoint,\n",
    "        d_model=d_model,\n",
    "        n_heads=n_heads,\n",
    "        n_layers=n_layers,\n",
    "        d_ff=d_ff,\n",
    "        dropout=config.model.dropout,\n",
    "        activation=config.model.activation,\n",
    "        prediction_horizon=config.data.prediction_horizon,\n",
    "    )\n",
    "    model.load_state_dict(state_dict)\n",
    "    print(\"Model loaded successfully.\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for test period\n",
    "from src.data.dataset import StockDataset, create_sequences\n",
    "\n",
    "# Extract features and target from test_df\n",
    "test_data = test_df[feature_columns].values\n",
    "test_targets = test_df[\"close\"].values.reshape(-1, 1)\n",
    "\n",
    "# Create sequences manually (same logic as prepare_dataset)\n",
    "test_X, test_y = create_sequences(\n",
    "    np.column_stack([test_data, test_targets]),\n",
    "    config.data.context_length,\n",
    "    config.data.prediction_horizon,\n",
    ")\n",
    "\n",
    "# Extract features (remove target column)\n",
    "test_X = test_X[:, :, :-1]\n",
    "\n",
    "# Extract target properly based on prediction_horizon\n",
    "if config.data.prediction_horizon > 1:\n",
    "    test_y = test_y[:, :, -1]\n",
    "else:\n",
    "    test_y = test_y[:, -1, -1]\n",
    "    if test_y.ndim == 0:\n",
    "        test_y = test_y.reshape(-1, 1)\n",
    "\n",
    "# Create StockDataset\n",
    "test_dataset = StockDataset(\n",
    "    data=test_X,\n",
    "    targets=test_y,\n",
    "    context_length=config.data.context_length,\n",
    "    prediction_horizon=config.data.prediction_horizon,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "print(f\"Test dataset: {len(test_dataset)} samples\")\n",
    "print(f\"Context length: {config.data.context_length} days\")\n",
    "print(f\"Prediction horizon: {config.data.prediction_horizon} day(s)\")\n",
    "print(f\"Features shape: {test_X.shape}, Targets shape: {test_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for test period\n",
    "print(\"Generating predictions...\")\n",
    "predictions = []\n",
    "prices_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        features, targets = batch\n",
    "        preds = model(features)\n",
    "        predictions.append(preds.cpu().numpy())\n",
    "        prices_list.append(targets.cpu().numpy())\n",
    "\n",
    "pred_next = np.concatenate(predictions).ravel()\n",
    "prices = np.concatenate(prices_list).ravel()\n",
    "\n",
    "# For backtest, we need prices and predictions aligned\n",
    "# Prices[i] is the price on day i, pred_next[i] is prediction for day i+1\n",
    "# So we need to align: use prices[context_length:] and pred_next[:-1] or similar\n",
    "# Actually, StockDataset returns targets that are prices at t+prediction_horizon\n",
    "# So prices[i] is price at day i+prediction_horizon relative to features[i]\n",
    "\n",
    "# For backtest, we need:\n",
    "# - prices: actual prices for each day we can trade\n",
    "# - predictions: predictions for next day price\n",
    "\n",
    "# StockDataset uses context_length days of features to predict price at t+prediction_horizon\n",
    "# So if we have N samples, we have predictions for days [context_length, context_length+N-1]\n",
    "# We need to extract actual prices for those days\n",
    "\n",
    "# Get actual prices from test_df\n",
    "test_prices = test_df[\"close\"].values\n",
    "context_length = config.data.context_length\n",
    "\n",
    "# Prices we can actually use for backtest (starting from context_length)\n",
    "prices_for_backtest = test_prices[context_length:]\n",
    "\n",
    "# Align predictions with prices\n",
    "# pred_next[i] predicts price at day context_length + i + prediction_horizon\n",
    "# But we want prediction for day context_length + i + 1\n",
    "# So we use pred_next[i] as prediction for prices_for_backtest[i+prediction_horizon-1]\n",
    "\n",
    "# Actually, simpler: if prediction_horizon=1, then pred_next[i] predicts prices_for_backtest[i]\n",
    "# But we need to shift: pred_next[i] predicts the NEXT price after prices_for_backtest[i]\n",
    "\n",
    "# Let's align properly:\n",
    "# - prices_for_backtest[0] is price on day context_length\n",
    "# - pred_next[0] predicts price on day context_length + prediction_horizon\n",
    "# So for backtest on day i, we use:\n",
    "#   current_price = prices_for_backtest[i]\n",
    "#   predicted_price = pred_next[i] (predicts price for day i+1 if prediction_horizon=1)\n",
    "\n",
    "# For simplicity, let's use:\n",
    "if len(pred_next) == len(prices_for_backtest):\n",
    "    # Perfect alignment\n",
    "    prices = prices_for_backtest\n",
    "    pred_next = pred_next\n",
    "elif len(pred_next) == len(prices_for_backtest) - config.data.prediction_horizon:\n",
    "    # Need to shift\n",
    "    prices = prices_for_backtest[config.data.prediction_horizon:]\n",
    "    pred_next = pred_next\n",
    "else:\n",
    "    # Use minimum length\n",
    "    min_len = min(len(pred_next), len(prices_for_backtest))\n",
    "    prices = prices_for_backtest[:min_len]\n",
    "    pred_next = pred_next[:min_len]\n",
    "\n",
    "print(f\"\\nBacktest data:\")\n",
    "print(f\"  Prices: {len(prices)} days\")\n",
    "print(f\"  Predictions: {len(pred_next)} days\")\n",
    "print(f\"  Price range: [{prices.min():.4f}, {prices.max():.4f}]\")\n",
    "print(f\"  Prediction range: [{pred_next.min():.4f}, {pred_next.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run backtest simulation\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BACKTEST ON NEW TICKER: {NEW_TICKER}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "engine = BacktestEngine(\n",
    "    initial_capital=initial_capital,\n",
    "    position_size_pct=position_size_pct,\n",
    "    entry_threshold_pct=entry_threshold_pct,\n",
    "    exit_threshold_pct=exit_threshold_pct,\n",
    "    commission_pct=commission_pct,\n",
    ")\n",
    "\n",
    "result = engine.run(prices=prices, predictions=pred_next)\n",
    "\n",
    "metrics = compute_metrics(\n",
    "    result,\n",
    "    initial_capital=initial_capital,\n",
    "    risk_free_rate_annual=risk_free_rate_annual,\n",
    "    prices=prices,\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BACKTEST METRICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total return:        {metrics.total_return_pct:.2f}%\")\n",
    "print(f\"Sharpe (annual):     {metrics.sharpe_ratio_annual:.3f}\")\n",
    "print(f\"Max drawdown:        {metrics.max_drawdown_pct:.2f}%\")\n",
    "print(f\"Number of trades:    {metrics.num_trades}\")\n",
    "if metrics.buy_and_hold_return_pct is not None:\n",
    "    print(f\"Buy & hold return:   {metrics.buy_and_hold_return_pct:.2f}%\")\n",
    "if metrics.excess_return_vs_bh_pct is not None:\n",
    "    print(f\"Excess vs B&H:       {metrics.excess_return_vs_bh_pct:+.2f}%\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show first few trades\n",
    "print(f\"\\nFirst 10 trades:\")\n",
    "for i, trade in enumerate(result.trades[:10]):\n",
    "    print(f\"  Trade {i+1}: Day {trade.date_idx} - {trade.side.upper()} {trade.quantity:.2f} shares @ {trade.price:.4f}\")\n",
    "\n",
    "if len(result.trades) > 10:\n",
    "    print(f\"  ... and {len(result.trades) - 10} more trades\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.plot(result.equity_curve, label=\"Strategy\", linewidth=2)\n",
    "# Buy & hold: for normalized prices, use relative change\n",
    "pct_change_cumulative = (prices - prices[0]) / np.abs(prices[0])\n",
    "bh_value = initial_capital * (1 + pct_change_cumulative)\n",
    "ax1.plot(bh_value, label=\"Buy & hold\", alpha=0.8, linewidth=2)\n",
    "ax1.set_ylabel(\"Portfolio value\", fontsize=12)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_title(f\"Equity Curve - {NEW_TICKER} (Unseen Ticker)\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "ax2 = axes[1]\n",
    "x = np.arange(len(result.equity_curve))\n",
    "peak = np.maximum.accumulate(result.equity_curve)\n",
    "safe_peak = np.where(peak <= 0, np.nan, peak)\n",
    "dd = 100 * (peak - result.equity_curve) / safe_peak\n",
    "dd = np.nan_to_num(dd, nan=0.0)\n",
    "ax2.plot(x, dd, label=\"Strategy\", color=\"C0\", linewidth=2)\n",
    "ax2.fill_between(x, dd, 0, alpha=0.3, color=\"C0\")\n",
    "bh_value_dd = initial_capital * (1 + (prices - prices[0]) / np.abs(prices[0]))\n",
    "peak_bh = np.maximum.accumulate(bh_value_dd)\n",
    "safe_peak_bh = np.where(peak_bh <= 0, np.nan, peak_bh)\n",
    "dd_bh = 100 * (peak_bh - bh_value_dd) / safe_peak_bh\n",
    "dd_bh = np.nan_to_num(dd_bh, nan=0.0)\n",
    "ax2.plot(x, dd_bh, label=\"Buy & hold\", color=\"C1\", alpha=0.8, linewidth=2)\n",
    "ax2.fill_between(x, dd_bh, 0, alpha=0.2, color=\"C1\")\n",
    "ax2.set_ylabel(\"Drawdown %\", fontsize=12)\n",
    "ax2.set_xlabel(\"Day\", fontsize=12)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_title(\"Drawdown\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SUMMARY: Model tested on {NEW_TICKER} (unseen during training)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Strategy return: {metrics.total_return_pct:.2f}%\")\n",
    "print(f\"Buy & hold return: {metrics.buy_and_hold_return_pct:.2f}%\" if metrics.buy_and_hold_return_pct else \"N/A\")\n",
    "print(f\"Excess return: {metrics.excess_return_vs_bh_pct:+.2f}%\" if metrics.excess_return_vs_bh_pct else \"N/A\")\n",
    "print(f\"Number of trades: {metrics.num_trades}\")\n",
    "print(f\"\\nThis shows how well the model generalizes to new stocks!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}