{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сравнение на Базов и Подобрен Модел\n",
    "\n",
    "Този notebook сравнява:\n",
    "- **Базов модел**: `StockTransformer` (само технически features)\n",
    "- **Подобрен модел**: `StockTransformerWithNews` (технически features + news embeddings)\n",
    "\n",
    "## Метрики за сравнение:\n",
    "- MAE (Mean Absolute Error)\n",
    "- RMSE (Root Mean Squared Error)\n",
    "- MAPE (Mean Absolute Percentage Error)\n",
    "- R² (R-squared)\n",
    "- Directional Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import os\n",
    "os.chdir(project_root)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.data.pipeline import get_datasets\n",
    "from src.data.pipeline_with_news import get_datasets_with_news\n",
    "from src.models.transformer_model import StockTransformer\n",
    "from src.models.transformer_model_with_news import StockTransformerWithNews\n",
    "from src.evaluation.metrics import calculate_metrics\n",
    "from src.utils.config import load_config\n",
    "\n",
    "config = load_config()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Tickers: {config.data.tickers}\")\n",
    "print(f\"Context length: {config.data.context_length}\")\n",
    "print(f\"Prediction horizon: {config.data.prediction_horizon}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model datasets (without news)\n",
    "print(\"\\n Loading BASE model datasets (without news)...\")\n",
    "config.data.use_news = False\n",
    "train_dataset_base, val_dataset_base, test_dataset_base, feature_columns = get_datasets(\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(f\" Base datasets loaded!\")\n",
    "print(f\"  Train: {len(train_dataset_base)} samples\")\n",
    "print(f\"  Val: {len(val_dataset_base)} samples\")\n",
    "print(f\"  Test: {len(test_dataset_base)} samples\")\n",
    "print(f\"  Features: {len(feature_columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load enhanced model datasets (with news)\n",
    "print(\"\\n Loading ENHANCED model datasets (with news)...\")\n",
    "config.data.use_news = True\n",
    "train_dataset_news, val_dataset_news, test_dataset_news, feature_columns_news = get_datasets_with_news(\n",
    "    config=config,\n",
    "    use_news_cache=True,\n",
    "    force_refresh_news=False\n",
    ")\n",
    "\n",
    "print(f\" Enhanced datasets loaded!\")\n",
    "print(f\"  Train: {len(train_dataset_news)} samples\")\n",
    "print(f\"  Val: {len(val_dataset_news)} samples\")\n",
    "print(f\"  Test: {len(test_dataset_news)} samples\")\n",
    "print(f\"  Features: {len(feature_columns_news)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "print(\"\\n Loading BASE model...\")\n",
    "from src.utils.config import PROJECT_ROOT\n",
    "\n",
    "# Try to load base model - check multiple possible locations\n",
    "base_model_paths = [\n",
    "    PROJECT_ROOT / \"models/checkpoints/best_model_base.pt\",  # Explicit base model\n",
    "    PROJECT_ROOT / \"models/checkpoints/best_model.pt\",  # Default location\n",
    "]\n",
    "\n",
    "base_model_path = None\n",
    "for path in base_model_paths:\n",
    "    if path.exists():\n",
    "        base_model_path = path\n",
    "        print(f\"  Found model at: {path}\")\n",
    "        break\n",
    "\n",
    "if base_model_path is None:\n",
    "    print(f\"  Base model not found in any of these locations:\")\n",
    "    for path in base_model_paths:\n",
    "        print(f\"    - {path}\")\n",
    "    print(\"  Will skip base model evaluation\")\n",
    "    base_model = None\n",
    "\n",
    "base_model = None\n",
    "if base_model_path.exists():\n",
    "    # Use weights_only=False because checkpoint may contain Config object\n",
    "    checkpoint = torch.load(base_model_path, map_location='cpu', weights_only=False)\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    \n",
    "    # Check if this is actually a base model or enhanced model\n",
    "    model_type = checkpoint.get('model_type', 'unknown')\n",
    "    \n",
    "    # Check if state_dict has news-related keys\n",
    "    has_news = any('news' in key for key in state_dict.keys())\n",
    "    \n",
    "    if has_news or model_type == 'StockTransformerWithNews':\n",
    "        print(f\"  Warning: {base_model_path} contains enhanced model, not base model\")\n",
    "        print(\"  Attempting to load as base model by filtering out news-related keys...\")\n",
    "        \n",
    "        # Try to load only base model parts (filter out news keys)\n",
    "        filtered_state_dict = {k: v for k, v in state_dict.items() if 'news' not in k}\n",
    "        \n",
    "        # Also need to adjust output_projection if it was for concat fusion\n",
    "        if 'output_projection.weight' in state_dict:\n",
    "            output_shape = state_dict['output_projection.weight'].shape\n",
    "            # If output_dim is large (concat fusion), we need to create new output_projection\n",
    "            if 'input_projection.weight' in filtered_state_dict:\n",
    "                inferred_d_model = filtered_state_dict['input_projection.weight'].shape[0]\n",
    "                # Create base model with correct d_model\n",
    "                base_model = StockTransformer(\n",
    "                    input_dim=len(feature_columns),\n",
    "                    d_model=inferred_d_model,\n",
    "                    n_heads=config.model.n_heads,\n",
    "                    n_layers=config.model.n_layers,\n",
    "                    d_ff=config.model.d_ff,\n",
    "                    dropout=config.model.dropout,\n",
    "                    activation=config.model.activation,\n",
    "                    prediction_horizon=config.data.prediction_horizon,\n",
    "                )\n",
    "                # Remove output_projection from filtered dict and let model use its own\n",
    "                filtered_state_dict.pop('output_projection.weight', None)\n",
    "                filtered_state_dict.pop('output_projection.bias', None)\n",
    "                \n",
    "                try:\n",
    "                    base_model.load_state_dict(filtered_state_dict, strict=False)\n",
    "                    base_model.eval()\n",
    "                    print(f\" Base model loaded (filtered from enhanced model)\")\n",
    "                    print(f\"  Validation loss: {checkpoint.get('score', 'N/A'):.6f}\")\n",
    "                    print(f\"  Note: Using base model architecture, news weights ignored\")\n",
    "                except Exception as e:\n",
    "                    print(f\" Error loading filtered base model: {e}\")\n",
    "                    base_model = None\n",
    "            else:\n",
    "                base_model = None\n",
    "        else:\n",
    "            base_model = None\n",
    "    else:\n",
    "        # Try to infer model parameters from checkpoint\n",
    "        # Get d_model from output_projection shape (for base model, output_dim = d_model)\n",
    "        if 'output_projection.weight' in state_dict:\n",
    "            output_shape = state_dict['output_projection.weight'].shape\n",
    "            # For base model, output_projection is [prediction_horizon, d_model]\n",
    "            # But if it's [1, 256], d_model might be 256 or the output was concatenated\n",
    "            # Check input_projection instead\n",
    "            if 'input_projection.weight' in state_dict:\n",
    "                inferred_d_model = state_dict['input_projection.weight'].shape[0]\n",
    "            else:\n",
    "                inferred_d_model = config.model.d_model\n",
    "        else:\n",
    "            inferred_d_model = config.model.d_model\n",
    "        \n",
    "        base_model = StockTransformer(\n",
    "            input_dim=len(feature_columns),\n",
    "            d_model=inferred_d_model,\n",
    "            n_heads=config.model.n_heads,\n",
    "            n_layers=config.model.n_layers,\n",
    "            d_ff=config.model.d_ff,\n",
    "            dropout=config.model.dropout,\n",
    "            activation=config.model.activation,\n",
    "            prediction_horizon=config.data.prediction_horizon,\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Use strict=False to ignore news-related keys if any\n",
    "            base_model.load_state_dict(state_dict, strict=False)\n",
    "            base_model.eval()\n",
    "            print(f\" Base model loaded from {base_model_path}\")\n",
    "            print(f\"  Validation loss: {checkpoint.get('score', 'N/A'):.6f}\")\n",
    "            print(f\"  Inferred d_model: {inferred_d_model}\")\n",
    "        except Exception as e:\n",
    "            print(f\" Error loading base model: {e}\")\n",
    "            print(\"  Will skip base model evaluation\")\n",
    "            base_model = None\n",
    "else:\n",
    "    if base_model_path is not None:\n",
    "        print(f\"  Base model not found at {base_model_path}\")\n",
    "    print(\"  Will skip base model evaluation\")\n",
    "    base_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load enhanced model\n",
    "print(\"\\n Loading ENHANCED model...\")\n",
    "from src.utils.config import PROJECT_ROOT\n",
    "enhanced_model_path = PROJECT_ROOT / \"models/checkpoints/best_model_with_news.pt\"\n",
    "\n",
    "enhanced_model = None\n",
    "if enhanced_model_path.exists():\n",
    "    # Use weights_only=False because checkpoint may contain Config object\n",
    "    checkpoint = torch.load(enhanced_model_path, map_location='cpu', weights_only=False)\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    \n",
    "    # Try to infer model parameters from checkpoint\n",
    "    # Get d_model from input_projection\n",
    "    if 'input_projection.weight' in state_dict:\n",
    "        inferred_d_model = state_dict['input_projection.weight'].shape[0]\n",
    "    elif 'encoder.layers.0.self_attention.q_proj.weight' in state_dict:\n",
    "        # Infer from attention layer\n",
    "        inferred_d_model = state_dict['encoder.layers.0.self_attention.q_proj.weight'].shape[0]\n",
    "    else:\n",
    "        inferred_d_model = config.model.d_model\n",
    "    \n",
    "    # Check output_projection to determine fusion method\n",
    "    if 'output_projection.weight' in state_dict:\n",
    "        output_shape = state_dict['output_projection.weight'].shape\n",
    "        # If output_dim > d_model, likely using concat fusion\n",
    "        # For concat: output_dim = d_model + news_proj_dim (usually d_model)\n",
    "        # So output_dim should be around 2 * d_model for concat\n",
    "        if output_shape[1] > inferred_d_model * 1.5:\n",
    "            inferred_fusion = \"concat\"\n",
    "        else:\n",
    "            inferred_fusion = \"add\"\n",
    "    else:\n",
    "        inferred_fusion = \"concat\"\n",
    "    \n",
    "    # Use feature_columns_news if available, otherwise use feature_columns\n",
    "    # (they should be the same for technical features, news embeddings are separate)\n",
    "    input_dim = len(feature_columns_news) if 'feature_columns_news' in globals() else len(feature_columns)\n",
    "    \n",
    "    enhanced_model = StockTransformerWithNews(\n",
    "        input_dim=input_dim,\n",
    "        news_embedding_dim=768,\n",
    "        d_model=inferred_d_model,\n",
    "        n_heads=config.model.n_heads,\n",
    "        n_layers=config.model.n_layers,\n",
    "        d_ff=config.model.d_ff,\n",
    "        dropout=config.model.dropout,\n",
    "        activation=config.model.activation,\n",
    "        prediction_horizon=config.data.prediction_horizon,\n",
    "        news_fusion_method=inferred_fusion,\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        enhanced_model.load_state_dict(state_dict, strict=False)\n",
    "        enhanced_model.eval()\n",
    "        print(f\" Enhanced model loaded from {enhanced_model_path}\")\n",
    "        print(f\"  Validation loss: {checkpoint.get('score', 'N/A'):.6f}\")\n",
    "        print(f\"  Inferred d_model: {inferred_d_model}\")\n",
    "        print(f\"  Inferred fusion method: {inferred_fusion}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error loading enhanced model: {e}\")\n",
    "        print(\"  Will skip enhanced model evaluation\")\n",
    "        enhanced_model = None\n",
    "else:\n",
    "    print(f\"  Enhanced model not found at {enhanced_model_path}\")\n",
    "    print(\"  Will skip enhanced model evaluation\")\n",
    "    enhanced_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate base model on test set\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUATING BASE MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "base_predictions = []\n",
    "base_targets = []\n",
    "\n",
    "if base_model is not None:\n",
    "    test_loader_base = DataLoader(\n",
    "        test_dataset_base,\n",
    "        batch_size=config.training.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    base_model.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader_base:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            pred = base_model(batch_x)\n",
    "            \n",
    "            base_predictions.append(pred.cpu().numpy())\n",
    "            base_targets.append(batch_y.cpu().numpy())\n",
    "    \n",
    "    base_predictions = np.concatenate(base_predictions, axis=0)\n",
    "    base_targets = np.concatenate(base_targets, axis=0)\n",
    "    \n",
    "    # Flatten if needed\n",
    "    if base_predictions.ndim > 1:\n",
    "        base_predictions = base_predictions.flatten()\n",
    "    if base_targets.ndim > 1:\n",
    "        base_targets = base_targets.flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    base_metrics = calculate_metrics(base_targets, base_predictions)\n",
    "    \n",
    "    print(\"\\n Base Model Metrics:\")\n",
    "    for metric_name, metric_value in base_metrics.items():\n",
    "        print(f\"  {metric_name}: {metric_value:.6f}\")\n",
    "else:\n",
    "    base_predictions = None\n",
    "    base_targets = None\n",
    "    base_metrics = None\n",
    "    print(\"  Base model not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate enhanced model on test set\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUATING ENHANCED MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "enhanced_predictions = []\n",
    "enhanced_targets = []\n",
    "\n",
    "if enhanced_model is not None:\n",
    "    test_loader_news = DataLoader(\n",
    "        test_dataset_news,\n",
    "        batch_size=config.training.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    enhanced_model.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader_news:\n",
    "            if len(batch) == 3:\n",
    "                batch_x, news_emb, batch_y = batch\n",
    "                news_emb = news_emb.to(device) if news_emb is not None else None\n",
    "            else:\n",
    "                batch_x, batch_y = batch\n",
    "                news_emb = None\n",
    "            \n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            if news_emb is not None:\n",
    "                pred = enhanced_model(batch_x, news_embeddings=news_emb)\n",
    "            else:\n",
    "                pred = enhanced_model(batch_x)\n",
    "            \n",
    "            enhanced_predictions.append(pred.cpu().numpy())\n",
    "            enhanced_targets.append(batch_y.cpu().numpy())\n",
    "    \n",
    "    enhanced_predictions = np.concatenate(enhanced_predictions, axis=0)\n",
    "    enhanced_targets = np.concatenate(enhanced_targets, axis=0)\n",
    "    \n",
    "    # Flatten if needed\n",
    "    if enhanced_predictions.ndim > 1:\n",
    "        enhanced_predictions = enhanced_predictions.flatten()\n",
    "    if enhanced_targets.ndim > 1:\n",
    "        enhanced_targets = enhanced_targets.flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    enhanced_metrics = calculate_metrics(enhanced_targets, enhanced_predictions)\n",
    "    \n",
    "    print(\"\\n Enhanced Model Metrics:\")\n",
    "    for metric_name, metric_value in enhanced_metrics.items():\n",
    "        print(f\"  {metric_name}: {metric_value:.6f}\")\n",
    "else:\n",
    "    enhanced_predictions = None\n",
    "    enhanced_targets = None\n",
    "    enhanced_metrics = None\n",
    "    print(\"  Enhanced model not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare metrics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if base_metrics is not None and enhanced_metrics is not None:\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Base Model': base_metrics,\n",
    "        'Enhanced Model': enhanced_metrics,\n",
    "    })\n",
    "    \n",
    "    # Calculate improvement\n",
    "    comparison_df['Improvement'] = comparison_df['Base Model'] - comparison_df['Enhanced Model']\n",
    "    comparison_df['Improvement %'] = (\n",
    "        (comparison_df['Base Model'] - comparison_df['Enhanced Model']) / \n",
    "        comparison_df['Base Model'].abs() * 100\n",
    "    )\n",
    "    \n",
    "    # For metrics where lower is better (MAE, RMSE, MAPE), positive improvement is good\n",
    "    # For metrics where higher is better (R², Directional Accuracy), negative improvement is good\n",
    "    # So we need to adjust the sign\n",
    "    for metric in ['R²', 'directional_accuracy']:\n",
    "        if metric in comparison_df.index:\n",
    "            comparison_df.loc[metric, 'Improvement'] = -comparison_df.loc[metric, 'Improvement']\n",
    "            comparison_df.loc[metric, 'Improvement %'] = -comparison_df.loc[metric, 'Improvement %']\n",
    "    \n",
    "    print(\"\\n Metrics Comparison:\")\n",
    "    print(comparison_df.to_string())\n",
    "    \n",
    "    print(\"\\n Interpretation:\")\n",
    "    print(\"  - For MAE, RMSE, MAPE: Negative 'Improvement' = Better (lower error)\")\n",
    "    print(\"  - For R², Directional Accuracy: Positive 'Improvement' = Better (higher score)\")\n",
    "    \n",
    "elif base_metrics is not None:\n",
    "    print(\"\\n Only Base Model available:\")\n",
    "    print(pd.Series(base_metrics).to_string())\n",
    "elif enhanced_metrics is not None:\n",
    "    print(\"\\n Only Enhanced Model available:\")\n",
    "    print(pd.Series(enhanced_metrics).to_string())\n",
    "else:\n",
    "    print(\"\\n  No models available for comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "if base_predictions is not None and enhanced_predictions is not None:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Sample subset for visualization (first 200 points)\n",
    "    n_samples = min(200, len(base_predictions))\n",
    "    indices = np.arange(n_samples)\n",
    "    \n",
    "    # Plot 1: Predictions vs Actual (Base Model)\n",
    "    axes[0, 0].plot(indices, base_targets[:n_samples], label='Actual', alpha=0.7, linewidth=2)\n",
    "    axes[0, 0].plot(indices, base_predictions[:n_samples], label='Predicted (Base)', alpha=0.7, linewidth=2)\n",
    "    axes[0, 0].set_title('Base Model: Predictions vs Actual')\n",
    "    axes[0, 0].set_xlabel('Sample Index')\n",
    "    axes[0, 0].set_ylabel('Price')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Predictions vs Actual (Enhanced Model)\n",
    "    axes[0, 1].plot(indices, enhanced_targets[:n_samples], label='Actual', alpha=0.7, linewidth=2)\n",
    "    axes[0, 1].plot(indices, enhanced_predictions[:n_samples], label='Predicted (Enhanced)', alpha=0.7, linewidth=2)\n",
    "    axes[0, 1].set_title('Enhanced Model: Predictions vs Actual')\n",
    "    axes[0, 1].set_xlabel('Sample Index')\n",
    "    axes[0, 1].set_ylabel('Price')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Scatter plot (Base Model)\n",
    "    axes[1, 0].scatter(base_targets[:n_samples], base_predictions[:n_samples], alpha=0.5, s=10)\n",
    "    axes[1, 0].plot([base_targets.min(), base_targets.max()], \n",
    "                    [base_targets.min(), base_targets.max()], \n",
    "                    'r--', linewidth=2, label='Perfect Prediction')\n",
    "    axes[1, 0].set_title(f'Base Model: Scatter Plot (R² = {base_metrics.get(\"R²\", 0):.4f})')\n",
    "    axes[1, 0].set_xlabel('Actual')\n",
    "    axes[1, 0].set_ylabel('Predicted')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Scatter plot (Enhanced Model)\n",
    "    axes[1, 1].scatter(enhanced_targets[:n_samples], enhanced_predictions[:n_samples], alpha=0.5, s=10)\n",
    "    axes[1, 1].plot([enhanced_targets.min(), enhanced_targets.max()], \n",
    "                     [enhanced_targets.min(), enhanced_targets.max()], \n",
    "                     'r--', linewidth=2, label='Perfect Prediction')\n",
    "    axes[1, 1].set_title(f'Enhanced Model: Scatter Plot (R² = {enhanced_metrics.get(\"R²\", 0):.4f})')\n",
    "    axes[1, 1].set_xlabel('Actual')\n",
    "    axes[1, 1].set_ylabel('Predicted')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/plots/model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"\\n Comparison plots saved to results/plots/model_comparison.png\")\n",
    "    plt.show()\n",
    "    \n",
    "elif base_predictions is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    n_samples = min(200, len(base_predictions))\n",
    "    indices = np.arange(n_samples)\n",
    "    \n",
    "    axes[0].plot(indices, base_targets[:n_samples], label='Actual', alpha=0.7, linewidth=2)\n",
    "    axes[0].plot(indices, base_predictions[:n_samples], label='Predicted', alpha=0.7, linewidth=2)\n",
    "    axes[0].set_title('Base Model: Predictions vs Actual')\n",
    "    axes[0].set_xlabel('Sample Index')\n",
    "    axes[0].set_ylabel('Price')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].scatter(base_targets[:n_samples], base_predictions[:n_samples], alpha=0.5, s=10)\n",
    "    axes[1].plot([base_targets.min(), base_targets.max()], \n",
    "                 [base_targets.min(), base_targets.max()], \n",
    "                 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    axes[1].set_title(f'Base Model: Scatter Plot (R² = {base_metrics.get(\"R²\", 0):.4f})')\n",
    "    axes[1].set_xlabel('Actual')\n",
    "    axes[1].set_ylabel('Predicted')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "elif enhanced_predictions is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    n_samples = min(200, len(enhanced_predictions))\n",
    "    indices = np.arange(n_samples)\n",
    "    \n",
    "    axes[0].plot(indices, enhanced_targets[:n_samples], label='Actual', alpha=0.7, linewidth=2)\n",
    "    axes[0].plot(indices, enhanced_predictions[:n_samples], label='Predicted', alpha=0.7, linewidth=2)\n",
    "    axes[0].set_title('Enhanced Model: Predictions vs Actual')\n",
    "    axes[0].set_xlabel('Sample Index')\n",
    "    axes[0].set_ylabel('Price')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].scatter(enhanced_targets[:n_samples], enhanced_predictions[:n_samples], alpha=0.5, s=10)\n",
    "    axes[1].plot([enhanced_targets.min(), enhanced_targets.max()], \n",
    "                 [enhanced_targets.min(), enhanced_targets.max()], \n",
    "                 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    axes[1].set_title(f'Enhanced Model: Scatter Plot (R² = {enhanced_metrics.get(\"R²\", 0):.4f})')\n",
    "    axes[1].set_xlabel('Actual')\n",
    "    axes[1].set_ylabel('Predicted')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"  No predictions available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заключение\n",
    "\n",
    "Сравнени са двата модела на test set. Провери:\n",
    "- Кой модел има по-добри метрики?\n",
    "- Дали новините подобряват прогнозите?\n",
    "- Кой модел е по-добър за различни метрики?\n",
    "\n",
    "**Следващи стъпки:**\n",
    "- Тествай на различни тикъри\n",
    "- Направи backtesting с двата модела\n",
    "- Анализирай кога новините помагат най-много"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}